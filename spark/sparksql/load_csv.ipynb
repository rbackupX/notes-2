{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Loading a csv\"\n",
    "date: 2018-06-06\n",
    "type: technical_note\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The good majority of the data you work with when starting out with PySpark is saved in `csv` format. Getting it all under your fingers, however, is a bit tricker than you might expect if you, like me, find yourself coming from `pandas`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prelims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import pyspark\n",
    "sc = pyspark.SparkContext()\n",
    "\n",
    "spark = pyspark.sql.SparkSession(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset is recycled from [the Academy Award blogpost I did earlier this year](https://napsterinblue.github.io/blog/2018/01/12/best-picture-consolation-prizes/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fpath = '../data/movieData.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spoiler alert, figuring out the proper function call to load a csv is going to take some revisioning. Let's append arguments and values as we go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "movieArgs = dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`movieArgs` unpacks to nothing at the moment. Let's see what a vanilla `read.csv` call gets us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----------+----------------+--------+-------------+--------+-----------+----+----------+------------------+----+------+\n",
      "| _c0|        _c1|             _c2|     _c3|          _c4|     _c5|        _c6| _c7|       _c8|               _c9|_c10|  _c11|\n",
      "+----+-----------+----------------+--------+-------------+--------+-----------+----+----------+------------------+----+------+\n",
      "|Rank|WeeklyGross|PctChangeWkGross|Theaters|DeltaTheaters|  AvgRev|GrossToDate|Week|  Thursday|              name|year|Winner|\n",
      "|17.0|     967378|            null|    14.0|         null| 69098.0|     967378|   1|1990-11-18|dances with wolves|1990|  True|\n",
      "| 9.0|    3871641|           300.0|    14.0|         null|276546.0|    4839019|   2|1990-11-25|dances with wolves|1990|  True|\n",
      "| 3.0|   12547813|           224.0|  1048.0|       1034.0| 11973.0|   17386832|   3|1990-12-02|dances with wolves|1990|  True|\n",
      "| 4.0|    9246632|           -26.3|  1053.0|          5.0|  8781.0|   26633464|   4|1990-12-09|dances with wolves|1990|  True|\n",
      "+----+-----------+----------------+--------+-------------+--------+-----------+----+----------+------------------+----+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "movies = spark.read.csv(fpath, **movieArgs)\n",
    "movies.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay. So Spark expected *not* to see a header in the file. That's alright. We'll just tell it to look for one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----------+----------------+--------+-------------+--------+-----------+----+----------+------------------+----+------+\n",
      "|Rank|WeeklyGross|PctChangeWkGross|Theaters|DeltaTheaters|  AvgRev|GrossToDate|Week|  Thursday|              name|year|Winner|\n",
      "+----+-----------+----------------+--------+-------------+--------+-----------+----+----------+------------------+----+------+\n",
      "|17.0|     967378|            null|    14.0|         null| 69098.0|     967378|   1|1990-11-18|dances with wolves|1990|  True|\n",
      "| 9.0|    3871641|           300.0|    14.0|         null|276546.0|    4839019|   2|1990-11-25|dances with wolves|1990|  True|\n",
      "| 3.0|   12547813|           224.0|  1048.0|       1034.0| 11973.0|   17386832|   3|1990-12-02|dances with wolves|1990|  True|\n",
      "| 4.0|    9246632|           -26.3|  1053.0|          5.0|  8781.0|   26633464|   4|1990-12-09|dances with wolves|1990|  True|\n",
      "| 4.0|    7272350|           -21.4|  1051.0|         -2.0|  6919.0|   33905814|   5|1990-12-16|dances with wolves|1990|  True|\n",
      "+----+-----------+----------------+--------+-------------+--------+-----------+----+----------+------------------+----+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "movieArgs['header'] = True\n",
    "\n",
    "movies = spark.read.csv(fpath, **movieArgs)\n",
    "movies.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That looks better.\n",
    "\n",
    "`pandas` might struggle with the `Thursday` column. Did PySpark?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Rank', 'string'),\n",
       " ('WeeklyGross', 'string'),\n",
       " ('PctChangeWkGross', 'string'),\n",
       " ('Theaters', 'string'),\n",
       " ('DeltaTheaters', 'string'),\n",
       " ('AvgRev', 'string'),\n",
       " ('GrossToDate', 'string'),\n",
       " ('Week', 'string'),\n",
       " ('Thursday', 'string'),\n",
       " ('name', 'string'),\n",
       " ('year', 'string'),\n",
       " ('Winner', 'string')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, I suppose technically it didn't. You have to make an effort to struggle, yeah?\n",
    "\n",
    "Let's tell it to take a crack at figuring out the schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Rank', 'double'),\n",
       " ('WeeklyGross', 'int'),\n",
       " ('PctChangeWkGross', 'double'),\n",
       " ('Theaters', 'double'),\n",
       " ('DeltaTheaters', 'double'),\n",
       " ('AvgRev', 'double'),\n",
       " ('GrossToDate', 'int'),\n",
       " ('Week', 'int'),\n",
       " ('Thursday', 'timestamp'),\n",
       " ('name', 'string'),\n",
       " ('year', 'int'),\n",
       " ('Winner', 'boolean')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movieArgs['inferSchema'] = True\n",
    "\n",
    "movies = spark.read.csv(fpath, **movieArgs)\n",
    "movies.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool. This is really coming along. Let's pull our data down to do some local analysis in `pandas`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Cannot convert tz-naive Timestamp, use tz_localize to localize",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-4d7b35f0345e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmovies\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoPandas\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\opt\\Spark\\spark-2.3.0-bin-hadoop2.7\\python\\pyspark\\sql\\dataframe.py\u001b[0m in \u001b[0;36mtoPandas\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1989\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfield\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataType\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTimestampType\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1990\u001b[0m                         \u001b[0mpdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfield\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1991\u001b[1;33m                             \u001b[0m_check_series_convert_timestamps_local_tz\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfield\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimezone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1992\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mpdf\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1993\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\opt\\Spark\\spark-2.3.0-bin-hadoop2.7\\python\\pyspark\\sql\\types.py\u001b[0m in \u001b[0;36m_check_series_convert_timestamps_local_tz\u001b[1;34m(s, timezone)\u001b[0m\n\u001b[0;32m   1835\u001b[0m     \u001b[1;33m:\u001b[0m\u001b[1;32mreturn\u001b[0m \u001b[0mpandas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSeries\u001b[0m \u001b[0mwhere\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mit\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0ma\u001b[0m \u001b[0mtimestamp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhas\u001b[0m \u001b[0mbeen\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0mto\u001b[0m \u001b[0mtz\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mnaive\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1836\u001b[0m     \"\"\"\n\u001b[1;32m-> 1837\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_check_series_convert_timestamps_localize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimezone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1838\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1839\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\opt\\Spark\\spark-2.3.0-bin-hadoop2.7\\python\\pyspark\\sql\\types.py\u001b[0m in \u001b[0;36m_check_series_convert_timestamps_localize\u001b[1;34m(s, from_timezone, to_timezone)\u001b[0m\n\u001b[0;32m   1821\u001b[0m         \u001b[1;31m# `s.dt.tz_localize('tzlocal()')` doesn't work properly when including NaT.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1822\u001b[0m         return s.apply(\n\u001b[1;32m-> 1823\u001b[1;33m             \u001b[1;32mlambda\u001b[0m \u001b[0mts\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mts\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtz_localize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfrom_tz\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mambiguous\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtz_convert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mto_tz\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtz_localize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1824\u001b[0m             if ts is not pd.NaT else pd.NaT)\n\u001b[0;32m   1825\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Nick\\Anaconda3\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self, func, convert_dtype, args, **kwds)\u001b[0m\n\u001b[0;32m   2353\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2354\u001b[0m                 \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masobject\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2355\u001b[1;33m                 \u001b[0mmapped\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2356\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2357\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSeries\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/src\\inference.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mC:\\opt\\Spark\\spark-2.3.0-bin-hadoop2.7\\python\\pyspark\\sql\\types.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(ts)\u001b[0m\n\u001b[0;32m   1822\u001b[0m         return s.apply(\n\u001b[0;32m   1823\u001b[0m             \u001b[1;32mlambda\u001b[0m \u001b[0mts\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mts\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtz_localize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfrom_tz\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mambiguous\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtz_convert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mto_tz\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtz_localize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1824\u001b[1;33m             if ts is not pd.NaT else pd.NaT)\n\u001b[0m\u001b[0;32m   1825\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1826\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/tslib.pyx\u001b[0m in \u001b[0;36mpandas._libs.tslib.Timestamp.tz_convert (pandas\\_libs\\tslib.c:13875)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: Cannot convert tz-naive Timestamp, use tz_localize to localize"
     ]
    }
   ],
   "source": [
    "df = movies.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fear Not"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's certainly not an inviting error message. [I wrestled with it myself less than an hour ago](https://napsterinblue.github.io/notes/spark/sparksql/topandas_datetime_error/).\n",
    "\n",
    "Here's a usable-enough workaround I found to finish this out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "movies = movies.withColumn('Thursday', movies['Thursday'].cast('string'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rank</th>\n",
       "      <th>WeeklyGross</th>\n",
       "      <th>PctChangeWkGross</th>\n",
       "      <th>Theaters</th>\n",
       "      <th>DeltaTheaters</th>\n",
       "      <th>AvgRev</th>\n",
       "      <th>GrossToDate</th>\n",
       "      <th>Week</th>\n",
       "      <th>Thursday</th>\n",
       "      <th>name</th>\n",
       "      <th>year</th>\n",
       "      <th>Winner</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17.0</td>\n",
       "      <td>967378</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>69098.0</td>\n",
       "      <td>967378</td>\n",
       "      <td>1</td>\n",
       "      <td>1990-11-18</td>\n",
       "      <td>dances with wolves</td>\n",
       "      <td>1990</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9.0</td>\n",
       "      <td>3871641</td>\n",
       "      <td>300.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>276546.0</td>\n",
       "      <td>4839019</td>\n",
       "      <td>2</td>\n",
       "      <td>1990-11-25</td>\n",
       "      <td>dances with wolves</td>\n",
       "      <td>1990</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.0</td>\n",
       "      <td>12547813</td>\n",
       "      <td>224.0</td>\n",
       "      <td>1048.0</td>\n",
       "      <td>1034.0</td>\n",
       "      <td>11973.0</td>\n",
       "      <td>17386832</td>\n",
       "      <td>3</td>\n",
       "      <td>1990-12-02</td>\n",
       "      <td>dances with wolves</td>\n",
       "      <td>1990</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.0</td>\n",
       "      <td>9246632</td>\n",
       "      <td>-26.3</td>\n",
       "      <td>1053.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>8781.0</td>\n",
       "      <td>26633464</td>\n",
       "      <td>4</td>\n",
       "      <td>1990-12-09</td>\n",
       "      <td>dances with wolves</td>\n",
       "      <td>1990</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.0</td>\n",
       "      <td>7272350</td>\n",
       "      <td>-21.4</td>\n",
       "      <td>1051.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>6919.0</td>\n",
       "      <td>33905814</td>\n",
       "      <td>5</td>\n",
       "      <td>1990-12-16</td>\n",
       "      <td>dances with wolves</td>\n",
       "      <td>1990</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Rank  WeeklyGross  PctChangeWkGross  Theaters  DeltaTheaters    AvgRev  \\\n",
       "0  17.0       967378               NaN      14.0            NaN   69098.0   \n",
       "1   9.0      3871641             300.0      14.0            NaN  276546.0   \n",
       "2   3.0     12547813             224.0    1048.0         1034.0   11973.0   \n",
       "3   4.0      9246632             -26.3    1053.0            5.0    8781.0   \n",
       "4   4.0      7272350             -21.4    1051.0           -2.0    6919.0   \n",
       "\n",
       "   GrossToDate  Week   Thursday                name  year  Winner  \n",
       "0       967378     1 1990-11-18  dances with wolves  1990    True  \n",
       "1      4839019     2 1990-11-25  dances with wolves  1990    True  \n",
       "2     17386832     3 1990-12-02  dances with wolves  1990    True  \n",
       "3     26633464     4 1990-12-09  dances with wolves  1990    True  \n",
       "4     33905814     5 1990-12-16  dances with wolves  1990    True  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = movies.toPandas()\n",
    "df['Thursday'] = pd.to_datetime(df['Thursday'])\n",
    "df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
