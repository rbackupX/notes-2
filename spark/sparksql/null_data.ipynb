{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Working with NULL Data\"\n",
    "date: 2018-06-07\n",
    "type: technical_note\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Missing data is a routine part of any Data Scientist's day-to-day. It's so fundamental, in fact, that moving over to `PySpark` can feel a bit jarring because it's not quite as immediately intuitive as other tools.\n",
    "\n",
    "However, if you can keep in mind that because of the way everything's stored/partitioned, **PySpark only handles NULL values at the Row-level**, things click a bit easier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some Spotty Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I went through the iris dataset and randomly injected a bunch of `NULL` values. Let's load it up (deliberately forcing each column to be read in as a `float` except the last column-- more on this later)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filepath = '../data/somenulls.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import pyspark\n",
    "sc = pyspark.SparkContext()\n",
    "\n",
    "spark = pyspark.sql.SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "schema = StructType([\n",
    "    StructField('a', FloatType(), True),\n",
    "    StructField('b', FloatType(), True),\n",
    "    StructField('c', FloatType(), True),\n",
    "    StructField('d', FloatType(), True),\n",
    "    StructField('e', StringType(), True),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+----+----+\n",
      "|   a|   b|   c|   d|   e|\n",
      "+----+----+----+----+----+\n",
      "| 5.1| 3.5| 1.4| 0.2|null|\n",
      "| 4.9| 3.0| 1.4| 0.2|null|\n",
      "| 4.7|null| 1.3| 0.2|null|\n",
      "| 4.6| 3.1| 1.5| 0.2|   0|\n",
      "|null| 3.6| 1.4| 0.2|   0|\n",
      "| 5.4| 3.9| 1.7| 0.4|   0|\n",
      "| 4.6| 3.4| 1.4|null|null|\n",
      "| 5.0| 3.4| 1.5| 0.2|null|\n",
      "| 4.4|null|null| 0.2|   0|\n",
      "| 4.9|null| 1.5| 0.1|   0|\n",
      "| 5.4|null| 1.5| 0.2|   0|\n",
      "| 4.8|null| 1.6| 0.2|   0|\n",
      "| 4.8| 3.0| 1.4|null|   0|\n",
      "| 4.3|null| 1.1|null|   0|\n",
      "| 5.8| 4.0| 1.2| 0.2|   0|\n",
      "|null| 4.4| 1.5| 0.4|   0|\n",
      "| 5.4| 3.9| 1.3| 0.4|   0|\n",
      "| 5.1| 3.5|null|null|   0|\n",
      "| 5.7| 3.8| 1.7| 0.3|   0|\n",
      "| 5.1| 3.8| 1.5| 0.3|   0|\n",
      "+----+----+----+----+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv(filepath, schema=schema,\n",
    "                    header=True)\n",
    "\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've got 150 records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "150"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tossing Out Rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `drop` function looks, *row-wise*, for any NULL values and removes the appropriate rows from our `DataFrame`, based on whatever strategy we want to employ. We can either:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop rows that have **any** NULL values with `how='any'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "68"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dropna(how='any').count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop rows that are **all** NULL with `how='all'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "149"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# only one row had all NULLs\n",
    "df.dropna(how='all').count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or set a threshold for the **minimum number of non-NULL values** allowed in a row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "126"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dropna(thresh=4).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also narrow our filtering criteria to particular rows, as some columns may have more significant NULL values than others.\n",
    "\n",
    "Here, we'll only drop rows if they have NULL values in the last 2 columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "109"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dropna(how='any', subset=['d', 'e']).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filling Missing Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`PySpark` doesn't come with a bunch of fancy data imputation methods, batteries-included-- that's more in the realm of the MLLib stack. But for the quick and dirty, let's start simple and look at the first 4 columns of the same `float` datatype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+---+---+\n",
      "|   a|   b|  c|  d|\n",
      "+----+----+---+---+\n",
      "| 5.1| 3.5|1.4|0.2|\n",
      "| 4.9| 3.0|1.4|0.2|\n",
      "| 4.7|null|1.3|0.2|\n",
      "| 4.6| 3.1|1.5|0.2|\n",
      "|null| 3.6|1.4|0.2|\n",
      "+----+----+---+---+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "floats = df.drop('e')\n",
    "\n",
    "floats.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to replace our missing values, we simply use the `fillna` function, passing in a number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+---+---+\n",
      "|     a|     b|  c|  d|\n",
      "+------+------+---+---+\n",
      "|   5.1|   3.5|1.4|0.2|\n",
      "|   4.9|   3.0|1.4|0.2|\n",
      "|   4.7|-999.0|1.3|0.2|\n",
      "|   4.6|   3.1|1.5|0.2|\n",
      "|-999.0|   3.6|1.4|0.2|\n",
      "+------+------+---+---+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "floats.fillna(-999).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and like the `dropna` method, we can specify which column(s) to pay attention to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+---+---+\n",
      "|   a|     b|  c|  d|\n",
      "+----+------+---+---+\n",
      "| 5.1|   3.5|1.4|0.2|\n",
      "| 4.9|   3.0|1.4|0.2|\n",
      "| 4.7|-999.0|1.3|0.2|\n",
      "| 4.6|   3.1|1.5|0.2|\n",
      "|null|   3.6|1.4|0.2|\n",
      "+----+------+---+---+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "floats.fillna(-999, subset=['b']).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, we can do the same thing by passing strings into categorical/string columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "|   e|\n",
      "+----+\n",
      "|null|\n",
      "|null|\n",
      "|null|\n",
      "|   0|\n",
      "|   0|\n",
      "+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "strings = df.select('e')\n",
    "strings.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|      e|\n",
      "+-------+\n",
      "|unknown|\n",
      "|unknown|\n",
      "|unknown|\n",
      "|      0|\n",
      "|      0|\n",
      "+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "strings.fillna('unknown').show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where this gets tricky is when we try to do both at the same time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PySpark will intuit that a number in `fillna` corresponds to the numeric columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+---+---+----+\n",
      "|    a|    b|  c|  d|   e|\n",
      "+-----+-----+---+---+----+\n",
      "|  5.1|  3.5|1.4|0.2|null|\n",
      "|  4.9|  3.0|1.4|0.2|null|\n",
      "|  4.7|-99.0|1.3|0.2|null|\n",
      "|  4.6|  3.1|1.5|0.2|   0|\n",
      "|-99.0|  3.6|1.4|0.2|   0|\n",
      "+-----+-----+---+---+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.fillna(-99).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and that a string means the string columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+---+---+----------+\n",
      "|   a|   b|  c|  d|         e|\n",
      "+----+----+---+---+----------+\n",
      "| 5.1| 3.5|1.4|0.2|who knows?|\n",
      "| 4.9| 3.0|1.4|0.2|who knows?|\n",
      "| 4.7|null|1.3|0.2|who knows?|\n",
      "| 4.6| 3.1|1.5|0.2|         0|\n",
      "|null| 3.6|1.4|0.2|         0|\n",
      "+----+----+---+---+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.fillna('who knows?').show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "but takes great offense when you try to do both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "value should be a float, int, long, string, bool or dict",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-68c95a523395>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfillna\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m99\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'who knows'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\opt\\spark\\spark-2.3.0-bin-hadoop2.7\\python\\pyspark\\sql\\dataframe.py\u001b[0m in \u001b[0;36mfillna\u001b[1;34m(self, value, subset)\u001b[0m\n\u001b[0;32m   1512\u001b[0m         \"\"\"\n\u001b[0;32m   1513\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlong\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbasestring\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1514\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"value should be a float, int, long, string, bool or dict\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1515\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1516\u001b[0m         \u001b[1;31m# Note that bool validates isinstance(int), but we don't want to\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: value should be a float, int, long, string, bool or dict"
     ]
    }
   ],
   "source": [
    "df.fillna([-99, 'who knows']).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or ignores you completely"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+---+---+----+\n",
      "|   a|   b|  c|  d|   e|\n",
      "+----+----+---+---+----+\n",
      "| 5.1| 3.5|1.4|0.2|null|\n",
      "| 4.9| 3.0|1.4|0.2|null|\n",
      "| 4.7|null|1.3|0.2|null|\n",
      "| 4.6| 3.1|1.5|0.2|   0|\n",
      "|null| 3.6|1.4|0.2|   0|\n",
      "+----+----+---+---+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.fillna(-99, 'who knows').show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can, however, chain them together!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+---+---+-------+\n",
      "|    a|    b|  c|  d|      e|\n",
      "+-----+-----+---+---+-------+\n",
      "|  5.1|  3.5|1.4|0.2|unknown|\n",
      "|  4.9|  3.0|1.4|0.2|unknown|\n",
      "|  4.7|-99.0|1.3|0.2|unknown|\n",
      "|  4.6|  3.1|1.5|0.2|      0|\n",
      "|-99.0|  3.6|1.4|0.2|      0|\n",
      "+-----+-----+---+---+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(df.fillna(-99)\n",
    "   .fillna('unknown').show(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing *Columns* with Missing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gets trickier when you're looking at the column-level.\n",
    "\n",
    "[We've got a whole other page explaining that.](https://napsterinblue.github.io/notes/spark/intermediate/dropping_columns/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
