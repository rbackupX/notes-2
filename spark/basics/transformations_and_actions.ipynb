{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Transformers and Actions\"\n",
    "date: 2018-05-23\n",
    "type: technical_note\n",
    "draft: false\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import pyspark\n",
    "sc = pyspark.SparkContext()\n",
    "\n",
    "zenPath = '../data/zen.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Put on your functional programing pants.\n",
    "\n",
    "Because Spark is more often than not used in the context of huge amounts of data, it only does expensive computation when absolutely needed. Before we get into the specifics, let's review the notion of *Lazy Evaluation*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lazy Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall in vanilla Python (post 3.X) that the `map` function returns a cryptic `map-object` when thrown up against some data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A bunch of really gross transformations takes virtually no time to map out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "441 ns ± 35.9 ns per loop (mean ± std. dev. of 7 runs, 1000000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "\n",
    "map(lambda x: str(hash(chr(ord(x)*2+100))**2)[:6], 'asdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But running this only returns a `map` object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<map at 0x8abec18>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "map(lambda x: str(hash(chr(ord(x)*2+100))**2)[:6], 'asdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To actually get the transformed output, we need to wrap the `map` iterable in a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['758179', '615470', '586868', '412387']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(map(lambda x: str(hash(chr(ord(x)*2+100))**2)[:6], 'asdf'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which takes considerably longer (relatively)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.42 µs ± 426 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "\n",
    "list(map(lambda x: str(hash(chr(ord(x)*2+100))**2)[:6], 'asdf'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chaining Logic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functionally\\*, this is the same thing as being more explicit with your incremental calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.16 µs ± 84.7 ns per loop (mean ± std. dev. of 7 runs, 1000000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "\n",
    "toOrd = lambda x: ord(x)\n",
    "timesTwo = lambda x: x * 2\n",
    "add100 = lambda x: x + 100\n",
    "hashIt = lambda x: hash(x)\n",
    "squareIt = lambda x: x ** 2\n",
    "toStr = lambda x: str(x)\n",
    "firstSix = lambda x: x[:6]\n",
    "\n",
    "allTogether = lambda x: firstSix(toStr(squareIt(hashIt(add100(timesTwo(toOrd(x)))))))\n",
    "\n",
    "map(allTogether, 'asdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But again, the bulk of the computation lies in the actual processing, not mapping out the sequence of calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12 µs ± 825 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "\n",
    "toOrd = lambda x: ord(x)\n",
    "timesTwo = lambda x: x * 2\n",
    "add100 = lambda x: x + 100\n",
    "hashIt = lambda x: hash(x)\n",
    "squareIt = lambda x: x ** 2\n",
    "toStr = lambda x: str(x)\n",
    "firstSix = lambda x: x[:6]\n",
    "\n",
    "allTogether = lambda x: firstSix(toStr(squareIt(hashIt(add100(timesTwo(toOrd(x)))))))\n",
    "\n",
    "list(map(allTogether, 'asdf'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformations and Actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark is entirely written in this fashion of \"planning\" and \"executing\" code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall our `lines` `RDD`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lines = sc.textFile(zenPath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can write all kinds of \"map-like\" calculations and they just return a bunch of additional RDD objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.PipelinedRDD"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shouldLines = lines.filter(lambda x: 'Python' in x)\n",
    "type(shouldLines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.PipelinedRDD"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "betterLines = lines.filter(lambda x: 'better' in x)\n",
    "type(betterLines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can chain together logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.PipelinedRDD"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "betterIsLines = betterLines.filter(lambda x: ' is ' in x)\n",
    "type(betterIsLines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But just as we wrapped our `map` object in `list` above, here we'll give the `RDD` an action to perform and return our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Beautiful is better than ugly.',\n",
       " 'Explicit is better than implicit.',\n",
       " 'Simple is better than complex.',\n",
       " 'Complex is better than complicated.',\n",
       " 'Flat is better than nested.',\n",
       " 'Sparse is better than dense.',\n",
       " 'Now is better than never.',\n",
       " 'Although never is often better than right now.']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "betterIsLines.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nothing actually executes until we give Spark an action to perform."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Runtime Deferral"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An interesting consequence of this is that we can spend a lot of time hacking away at code before we know that we're wrong."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark is more than happy to \"make you an RDD\" for a file that doesn't exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "badPath = sc.textFile('doesntexist.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And chain together complicated mapping behaviors from it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gamePlan = badPath.filter(lambda x: x)\n",
    "gamePlan = gamePlan.filter(lambda x: 'asdf' in x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only to tell you that it broke from the get-go when you actually try and retrieve some of your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred while calling o29.partitions.\n",
      ": org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: file:/C:/Users/nhounshell/Documents/github/BlackBook/Spark/Basics/doesntexist.txt\r\n",
      "\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:287)\r\n",
      "\tat org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:229)\r\n",
      "\tat org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:315)\r\n",
      "\tat org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:200)\r\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:253)\r\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:251)\r\n",
      "\tat scala.Option.getOrElse(Option.scala:121)\r\n",
      "\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:251)\r\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:35)\r\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:253)\r\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:251)\r\n",
      "\tat scala.Option.getOrElse(Option.scala:121)\r\n",
      "\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:251)\r\n",
      "\tat org.apache.spark.api.java.JavaRDDLike$class.partitions(JavaRDDLike.scala:61)\r\n",
      "\tat org.apache.spark.api.java.AbstractJavaRDDLike.partitions(JavaRDDLike.scala:45)\r\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n",
      "\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\r\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\r\n",
      "\tat java.lang.Thread.run(Unknown Source)\r\n",
      "\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    gamePlan.first()\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\*heh, puns"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
