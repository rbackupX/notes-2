{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Multi-Class Regression with SoftMax\"\n",
    "date: 2018-09-05\n",
    "type: technical_note\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note, these notes were taken in the context of Week 3 of [Improving Deep Neural Networks](https://www.coursera.org/learn/deep-neural-network)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When your prediction task extends beyond a binary classification, you want to rely less on the sigmoid function and logistic regression. While you might see some success doing it anyways, and then doing some `numpy.max()` dancing over your results, a much cleaner approach is to use the *SoftMax* function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Essentially, softmax takes an arbitrary results vector, `Z`, and instead of applying our typical sigmoid function to it, instead does the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Overwrites each value, `z_i` with `t_i`, where\n",
    "\n",
    "$t_i = e^{z_i}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Normalizes each value by the sum of all values in the vector (the activation function)\n",
    "\n",
    "$a = \\frac{e^Z}{\\sum{t_i}}$\n",
    "\n",
    "This has the convenient effect of all values in the vector `a` summing to 1-- a rough \"percent likelihood\" value assigned to each cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. In terms of training, we can do Gradient Descent on this just fine as the cost function is essentially the same as that for Logistic Regression, but with more parts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Simple Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Say we're trying to decide between 4 separate classes and wind up with a final vector that looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 5,  2, -1,  3])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "Z = np.array([5, 2, -1, 3])\n",
    "Z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Determining softmax likelihood is easy enough, following along with the steps above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([148.4131591 ,   7.3890561 ,   0.36787944,  20.08553692])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "T = np.exp(Z)\n",
    "T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.84203357, 0.04192238, 0.00208719, 0.11395685])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = T / np.sum(T)\n",
    "A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that `Class_0` having a large value makes it likely and conversely `Class_2` having a low value makes it unlikely, thus mirroring our Sigmoid Activation intuition."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
