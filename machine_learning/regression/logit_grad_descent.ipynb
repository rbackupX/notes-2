{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Logistic Regression Gradient Descent\"\n",
    "date: 2018-08-07\n",
    "type: technical_note\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Building Blocks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall our equation for the Cost Function of a Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\mathcal{L}(\\hat{y}, y) = -\\big(y\\log\\hat{y} + (1-y)\\log(1-\\hat{y})\\big)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the weights, `w`, our inputs, `x`, and a bias term, `b` to get a vector `z`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$z = w^{T} x + b$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we want this vector to be between `0` and `1`, so we pipe it through a sigmoid function, to get our predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\hat{y} = \\sigma(z)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We refer to the sigmoid function that runs over all of our values as the *activation function*, so for shorthand, we'll say"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$a = \\hat{y}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And thus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\mathcal{L}(a, y) = -\\big(y\\log a + (1-y)\\log(1 - a)\\big)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So if our aim is to minimize our overall cost, we need to lean on some calculus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Idea here is that we're going to take incremental steps across the inputs of cost function-- the weights and bias term, taking `x` as given. Which means that we want work out the derivative of the cost function *with respect to those terms*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Finding the Derivatives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the chain of execution to arrive at our cost function, we have:\n",
    "\n",
    "Our `z` as an intermediate value, generated as a function of `w`, `X`, and `b`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$z = w^{T}x + b$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`a`, which is a function of `z`, applied with the our standard sigmoid function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$a = \\hat{y} = \\sigma(z) = \\frac{1}{1 + e^{-z}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, *Loss* is a function of `y`, or true values, and `a` (all of *its* dependencies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\mathcal{L}(a, y) = -\\big(y\\log a + (1-y)\\log(1 - a)\\big)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're trying to Chain Rule our way backwards, so we need to figure out all of the partial derivatives that impact this loss function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Derivatives to take as Given"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hand-wavy derivations, courtesy of the *Logistic Regression Gradient Descent* video during Week 2 of [Neural Networks and Deep Learning](https://www.coursera.org/learn/neural-networks-deep-learning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sigmoid wrt `z`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\frac{\\delta a}{\\delta z} = a (1 - a)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Loss Function wrt `a`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\frac{\\delta \\mathcal {L}}{\\delta a} = -\\frac{y}{a} + \\frac{1-y}{1-a}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Applying the Chain Rule to our Loss Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\frac{\\delta \\mathcal {L}}{\\delta z} = \\frac{\\delta \\mathcal {L}}{\\delta a}\\frac{\\delta a}{\\delta z}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Substituting in"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\frac{\\delta \\mathcal {L}}{\\delta z} = \\big( -\\frac{y}{a} + \\frac{1-y}{1-a} \\big) a(1-a)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\frac{\\delta \\mathcal {L}}{\\delta z} = a-y$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extrapolating to weights and bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assuming a simple formula for `z` of the form\n",
    "\n",
    "$z = w_1 x_1 + w_2 x_2 + b$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can apply the same Chain Rule logic as above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### w_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\frac{\\delta \\mathcal{L}}{\\delta w_1} = \\frac{\\delta \\mathcal{L}}{\\delta z} \\frac{\\delta z}{\\delta w_1}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Substituting again"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\frac{\\delta \\mathcal{L}}{\\delta w_1} = (a - y) \\frac{\\delta z}{\\delta w_1}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\frac{\\delta \\mathcal{L}}{\\delta w_1} = (a - y) x_1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### w_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Follows the exact same form"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\frac{\\delta \\mathcal{L}}{\\delta w_2} = (a - y) x_2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Derivative just goes to 1 and cancels out the term"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\frac{\\delta \\mathcal{L}}{\\delta b} = (a - y)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## At Scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far we've been showing the cost of *one* training example. Now, when you consider all `m` training examples, your Cost Function looks like\n",
    "\n",
    "$J(w, b) = \\frac{1}{m} \\sum^{m}_{i=1} \\mathcal{L}\\big( a^{i}, y \\big)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And if you want to take the derivative of this, with respect to whatever, the fraction and the summation terms are going to get kicked out to the front, because mathâ„¢. This makes the calculation very tidy!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Descent Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set our parameters to `0`, by default"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``` python\n",
    "J, dw_1, dw_2, db = 0\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then this loop happens *for each training iteration step*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``` python\n",
    "# one pass for each of the m training examples\n",
    "for i in range(m):\n",
    "    z = np.dot(w, x) + b\n",
    "    a = sigma(z)\n",
    "    J += cost(y, a)\n",
    "    dz += a - y\n",
    "    dw_1 += x[1]*dz\n",
    "    dw_2 += x[1]*dz\n",
    "    db += dz\n",
    "\n",
    "# handle the leading fraction in the cost function\n",
    "J = J / m\n",
    "dw_1 = dw_1 / m\n",
    "dw_2 = dw_2 / m\n",
    "\n",
    "# adjust weights by learning rate\n",
    "w_1 = w_1 - alpha * dw_1\n",
    "w_2 = w_2 - alpha * dw_2\n",
    "b = b - alpha * b\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of this looping is, of course, wildly inefficient. Which is why we vectorize."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorized Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``` python\n",
    "J = b = 0\n",
    "w = np.zeros(n_x, 1)\n",
    "\n",
    "# Our iterations\n",
    "Z = np.dot(w.T, X) + b\n",
    "A = sigma(Z)\n",
    "\n",
    "dZ = A - Y\n",
    "\n",
    "dw = (1/m) * np.dot(X, dZ.T)\n",
    "db = (1/m) * np.sum(dZ)\n",
    "\n",
    "w = w - alpha * dw\n",
    "b = b - alpha * db\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: If we want to run `1000` iterations, we'd still have to wrap the third line down in a `for` loop"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
