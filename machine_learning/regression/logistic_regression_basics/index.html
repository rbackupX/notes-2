<!DOCTYPE html>
<html lang="en">

<head>

    
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-66582-32"></script>
    <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-66582-32');
    </script>

    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<meta property="og:title" content="Logistic Regression Basics" />
<meta property="og:description" content="Stated with variables Our goal is to find predictions that accurately predict the actual values
We&rsquo;ve got a bunch of input data
$x \in \mathbb{R}^{n}$
We&rsquo;ve got our 0 or 1 target
$y$
Our predictions between 0 and 1
$\hat{y}$
We&rsquo;ll arrive at our predictions using our weights
$w \in \mathbb{R}^{n}$
And our bias unit
$b \in \mathbb{R}$
Both of which will be a result of our computation
But we need to coerce our prediction values to be between 0 and 1, therefore we need a sigmoid function." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://napsterinblue.github.io/notes/machine_learning/regression/logistic_regression_basics/" />



<meta property="article:published_time" content="2018-08-07T00:00:00&#43;00:00"/>

<meta property="article:modified_time" content="2018-08-07T00:00:00&#43;00:00"/>











<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Logistic Regression Basics"/>
<meta name="twitter:description" content="Stated with variables Our goal is to find predictions that accurately predict the actual values
We&rsquo;ve got a bunch of input data
$x \in \mathbb{R}^{n}$
We&rsquo;ve got our 0 or 1 target
$y$
Our predictions between 0 and 1
$\hat{y}$
We&rsquo;ll arrive at our predictions using our weights
$w \in \mathbb{R}^{n}$
And our bias unit
$b \in \mathbb{R}$
Both of which will be a result of our computation
But we need to coerce our prediction values to be between 0 and 1, therefore we need a sigmoid function."/>
<meta name="generator" content="Hugo 0.40.3" />

    
<script type="application/ld+json">
{
  "@context": "http://schema.org",
  "@type": "BlogPosting",
  "headline": "Logistic Regression Basics",
  "url": "https://napsterinblue.github.io/notes/machine_learning/regression/logistic_regression_basics/",
  "wordCount": "451",
  "datePublished": "2018-08-07T00:00:00&#43;00:00",
  "dateModified": "2018-08-07T00:00:00&#43;00:00",
  "author": {
    "@type": "Person",
    "name": ""
  }
}
</script> 

    <title>Logistic Regression Basics</title>

    
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0-beta.2/css/bootstrap.min.css" integrity="sha384-PsH8R72JQ3SOdhVi3uxftmaW6Vc51MKb0q5P2rRUpPvrszuE4W1povHYgTpBfshb"
        crossorigin="anonymous">

    
    <link href="https://napsterinblue.github.io/notes/css/custom.css" rel="stylesheet">
    <link href="https://napsterinblue.github.io/notes/css/syntax.css" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css?family=Muli:400,500,700" rel="stylesheet">

    <link href="" rel="alternate" type="application/rss+xml" title="Data Science Notes" />

    <script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_CHTML">
</script>
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [['$','$'], ['\\(','\\)']],
    displayMath: [['$$','$$'], ['\[','\]']],
    processEscapes: true,
    processEnvironments: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre','code'],
    TeX: { equationNumbers: { autoNumber: "AMS" },
         extensions: ["AMSmath.js", "AMSsymbols.js"] }
  }
});
</script>

</head>

<body>

    <nav class="navbar navbar-expand-sm fixed-top">
        <div class="container-fluid">
            <a class="navbar-brand" href="https://napsterinblue.github.io">Movies, Metrics, Musings</a>
            <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarSupportedContent" aria-controls="navbarSupportedContent"
                aria-expanded="false" aria-label="Toggle navigation">
                <span class="navbar-toggler-icon"></span>
            </button>

            <div class="collapse navbar-collapse" id="navbarSupportedContent">
                <ul class="nav navbar-nav mr-auto"></ul>
                <ul class="nav navbar-nav">
                    <li><a href="https://napsterinblue.github.io/pages/about.html" title="About">About</a></li>
                    <li><a href="https://napsterinblue.github.io/archives.html" title="Archive">Archive</a></li>
                    <li><a href="https://napsterinblue.github.io/pages/resources.html" title="Resources">Resources</a></li>
                    <li><a href="https://napsterinblue.github.io/notes/" title="Notes">My Notes</a></li>

                </ul>
            </div>
        </div>
    </nav>


    
    <div class="container">
        <div class="row">
            <div class="col-sm-12">

                 


<article>
  <div class="technical_note">
  <header>
    <h1 class="technical_note_title">Logistic Regression Basics</h1>
    <div class="technical_note_date">
      <time datetime=" 2018-08-07T00:00:00Z "> 07 Aug 2018</time>
    </div>
  </header>
  <div class="content">
  

<h2 id="stated-with-variables">Stated with variables</h2>

<p>Our goal is to find predictions that accurately predict the actual values</p>

<p>We&rsquo;ve got a bunch of input data</p>

<p>$x \in \mathbb{R}^{n}$</p>

<p>We&rsquo;ve got our <code>0</code> or <code>1</code> target</p>

<p>$y$</p>

<p>Our predictions between <code>0</code> and <code>1</code></p>

<p>$\hat{y}$</p>

<p>We&rsquo;ll arrive at our predictions using our weights</p>

<p>$w \in \mathbb{R}^{n}$</p>

<p>And our bias unit</p>

<p>$b \in \mathbb{R}$</p>

<p>Both of which will be a result of our computation</p>

<p>But we need to coerce our prediction values to be between 0 and 1, therefore we need a <em>sigmoid function.</em></p>

<p>$\sigma(z) = \frac{1}{1+e^{-z}}$</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="o">%</span><span class="n">pylab</span> <span class="n">inline</span>

<span class="k">def</span> <span class="nf">sigmoid</span><span class="p">(</span><span class="n">z</span><span class="p">):</span>
    <span class="k">return</span> <span class="mi">1</span><span class="o">/</span><span class="p">(</span><span class="mi">1</span><span class="o">+</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">z</span><span class="p">))</span></code></pre></div>
<pre><code>Populating the interactive namespace from numpy and matplotlib
</code></pre>

<p>Because as you can see, the values tend to <code>0</code> for negative numbers, and <code>1</code> for positive numbers. Furthermore, the curve crosses <code>x=0</code> at <code>y=0.5</code>.</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">X</span><span class="p">))</span>
<span class="n">ax</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;k&#39;</span><span class="p">)</span></code></pre></div>
<pre><code>&lt;matplotlib.lines.Line2D at 0x8b6b7b8&gt;
</code></pre>

<p><img src="logistic_regression_basics_8_1.png" alt="png" /></p>

<h2 id="cost-function">Cost Function</h2>

<p>So our prediction vector is going to be a multiplication of the inputs $x_1, &hellip;, x_n$ by the weights $w_1, &hellip;, w_n$, plus a bias term $b$.</p>

<p>$\hat{y} = \sigma(w^{T}x + b) \quad \text{where} \quad \sigma(z) = \frac{1}{1+e^{-z}}$</p>

<p>Traditionally, we might consider some sort of cost function like squared error&ndash; the difference between observation and actual, squared.</p>

<p>$\mathcal{L}(\hat{y}, y) = \frac{1}{2}(\hat{y} - y)^{2}$</p>

<p>However, this leads to some very poorly-behaved curves. Instead, we use:</p>

<p>$\mathcal{L}(\hat{y}, y) = -\big(y\log\hat{y} + (1-y)\log(1-\hat{y})\big)$</p>

<h2 id="intution">Intution</h2>

<p>Recall the shape of the <code>log</code> function:</p>

<ul>
<li>It&rsquo;s basically negative infinity at <code>0</code></li>
<li>It is exactly <code>0</code> at <code>1</code></li>
<li>It scales (slowly) to positive infinity</li>
</ul>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mf">0.001</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">X</span><span class="p">))</span>
<span class="n">ax</span><span class="o">.</span><span class="n">axhline</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">)</span></code></pre></div>
<pre><code>&lt;matplotlib.lines.Line2D at 0x8f5ad68&gt;
</code></pre>

<p><img src="logistic_regression_basics_18_1.png" alt="png" /></p>

<p>So looking back at this cost function and considering the behavior of <code>log</code> consider what happens in the following scenarios</p>

<h3 id="if-y-1">If <code>y=1</code></h3>

<p>Our Loss Fucntion becomes</p>

<p>$\mathcal{L} = -\big( \log(\hat{y}) + 0\big)$</p>

<p>Therefore, if we predict <code>1</code>, then <code>log(1)</code> evalues to <code>0</code>&ndash; no error.</p>

<p>Conversely, if we predict <code>0</code>, then we have basically infinite error. We don&rsquo;t ever want to be certain that it&rsquo;s a <code>0</code> when it&rsquo;s actually not.</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mf">0.001</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">X</span><span class="p">))</span></code></pre></div>
<pre><code>[&lt;matplotlib.lines.Line2D at 0x8f344a8&gt;]
</code></pre>

<p><img src="logistic_regression_basics_24_1.png" alt="png" /></p>

<h3 id="if-y-0">If <code>y=0</code></h3>

<p>Our Loss Fucntion becomes</p>

<p>$\mathcal{L}(\hat{y}, y) = -\big(0 + (1)\log(1-\hat{y})\big)$</p>

<p>And looking at that last term, we see that as our prediction gets closer and closer to <code>1</code>, the error becomes infinite.</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mf">0.</span><span class="p">,</span> <span class="o">.</span><span class="mi">999</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">X</span><span class="p">))</span></code></pre></div>
<pre><code>[&lt;matplotlib.lines.Line2D at 0x910c358&gt;]
</code></pre>

<p><img src="logistic_regression_basics_29_1.png" alt="png" /></p>

<h2 id="cost-function-1">Cost Function</h2>

<p>If this intuition makes sense at a record-level, then extrapolating this <em>loss function</em> to each of our records helps us arrive at our <em>Cost Function</em>, expressed as</p>

<p>$J(w, b) = \frac{1}{m} \sum_{i=1}^{m} \mathcal{L}(\hat{y}^{i}, y^{i})$</p>

<p>$J(w, b) = - \frac{1}{m} \sum_{i=1}^{m} \big(y^{i}\log\hat{y}^{i} + (1-y^{i})\log(1-\hat{y}^{i})\big)$</p>

</div>
  <aside>
      <div class="bug_reporting">
          <h4>Find an error or bug?</h4>
          <p>Everything on this site is available on GitHub. Head to <a href='https://github.com/napsterinblue/notes/issues/new'>and submit a suggested change</a>. You can also message me directly on <a href='https://twitter.com/napsterinblue'>Twitter</a>.</p>
      </div>
      </aside>

    </div>
</article>




            </div>

        </div>
    </div>

    

    <footer class="footer text-center">
        <div class="container">
            <span class="text-muted">This project contains 113 pages and is available on <a href="https://github.com/napsterinblue/notes">GitHub</a>. Copyright &copy; NapsterInBlue, <time datetime="2018">2018</time>.</span>
        </div>
    </footer>

    <script src="https://code.jquery.com/jquery-3.2.1.slim.min.js" integrity="sha384-KJ3o2DKtIkvYIK3UENzmM7KCkRr/rE9/Qpg6aAZGJwFDMVNA/GpGFF93hXpG5KkN"
        crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.12.3/umd/popper.min.js" integrity="sha384-vFJXuSJphROIrBnz7yo7oB41mKfc8JzQZiCq4NCceLEaO4IHwicKwpJf9c9IpFgh"
        crossorigin="anonymous"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0-beta.2/js/bootstrap.min.js" integrity="sha384-alpBpkh1PFOepccYVYDB4do5UnbKysX5WZXm3XxPqe5iKTfUKjNkCk9SaVuEZflJ"
        crossorigin="anonymous"></script>

</body>

</html>
