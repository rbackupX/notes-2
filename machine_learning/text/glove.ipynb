{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"GloVe Embedding\"\n",
    "date: 2018-10-24\n",
    "type: technical_note\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we mentioned in the Word2Vec notebook, training your Embedding Matrix involves setting up some fake task for a Neural Network to optimize over.\n",
    "\n",
    "[Stanford's GloVe Embedding model](https://nlp.stanford.edu/projects/glove/) is very similar to the Word2Vec implementation, but with one crucial difference:\n",
    "\n",
    "GloVe places a higher importance on *frequency* of co-occurrence between two words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, an enormous `vocab_size x vocab_size` matrix is constructed as a result of a pass through of your entire corpus to get all unique words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, to reduce dimensionality, we look for a factorization that minimizes the following\n",
    "\n",
    "$\\sum_i \\sum_j f(Xij)(\\Theta^T_j e_j + b_i + b_j - log(Xij))^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where\n",
    "\n",
    "- `X_ij` is the number of times `i` appears in the context of `j` (say, proximity of 10 words)\n",
    "- `f()` is a *weighting term* that zeros out if the two words don't ever appear near each other.\n",
    "- `b_i` and `b_j` are bias terms at the word-level"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Runtime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because of the exhaustiveness of the co-occurrence matrix construction, GloVe involves a considerable up-front computation cost. This calculation, however, *does* lend itself to some pretty straight-forward parallelization."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
