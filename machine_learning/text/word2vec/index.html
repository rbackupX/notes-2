<!DOCTYPE html>
<html lang="en">

<head>

    
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-66582-32"></script>
    <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-66582-32');
    </script>

    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<meta property="og:title" content="Word2Vec" />
<meta property="og:description" content="As mentioned in the Word Embeddings notebook, there are many ways to train a Neural Network to produce a Word Embedding matrix for a given vocabulary.
One of the more popular implementations of this is TensorFlow&rsquo;s Word2Vec. This notebook should provide a high-level intuition of this training approach.
Fake Task The key takeaway for understanding how we fit an embedding layer is that we set our data up to solve an arbitrary problem when iterating over a corpus of text." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://napsterinblue.github.io/notes/machine_learning/text/word2vec/" />



<meta property="article:published_time" content="2018-10-24T00:00:00&#43;00:00"/>

<meta property="article:modified_time" content="2018-10-24T00:00:00&#43;00:00"/>











<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Word2Vec"/>
<meta name="twitter:description" content="As mentioned in the Word Embeddings notebook, there are many ways to train a Neural Network to produce a Word Embedding matrix for a given vocabulary.
One of the more popular implementations of this is TensorFlow&rsquo;s Word2Vec. This notebook should provide a high-level intuition of this training approach.
Fake Task The key takeaway for understanding how we fit an embedding layer is that we set our data up to solve an arbitrary problem when iterating over a corpus of text."/>
<meta name="generator" content="Hugo 0.40.3" />

    
<script type="application/ld+json">
{
  "@context": "http://schema.org",
  "@type": "BlogPosting",
  "headline": "Word2Vec",
  "url": "https://napsterinblue.github.io/notes/machine_learning/text/word2vec/",
  "wordCount": "384",
  "datePublished": "2018-10-24T00:00:00&#43;00:00",
  "dateModified": "2018-10-24T00:00:00&#43;00:00",
  "author": {
    "@type": "Person",
    "name": ""
  }
}
</script> 

    <title>Word2Vec</title>

    
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0-beta.2/css/bootstrap.min.css" integrity="sha384-PsH8R72JQ3SOdhVi3uxftmaW6Vc51MKb0q5P2rRUpPvrszuE4W1povHYgTpBfshb"
        crossorigin="anonymous">

    
    <link href="https://napsterinblue.github.io/notes/css/custom.css" rel="stylesheet">
    <link href="https://napsterinblue.github.io/notes/css/syntax.css" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css?family=Muli:400,500,700" rel="stylesheet">

    <link href="" rel="alternate" type="application/rss+xml" title="Data Science Notes" />

    <script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_CHTML">
</script>
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [['$','$'], ['\\(','\\)']],
    displayMath: [['$$','$$'], ['\[','\]']],
    processEscapes: true,
    processEnvironments: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre','code'],
    TeX: { equationNumbers: { autoNumber: "AMS" },
         extensions: ["AMSmath.js", "AMSsymbols.js"] }
  }
});
</script>

</head>

<body>

    <nav class="navbar navbar-expand-sm fixed-top">
        <div class="container-fluid">
            <a class="navbar-brand" href="https://napsterinblue.github.io">Movies, Metrics, Musings</a>
            <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarSupportedContent" aria-controls="navbarSupportedContent"
                aria-expanded="false" aria-label="Toggle navigation">
                <span class="navbar-toggler-icon"></span>
            </button>

            <div class="collapse navbar-collapse" id="navbarSupportedContent">
                <ul class="nav navbar-nav mr-auto"></ul>
                <ul class="nav navbar-nav">
                    <li><a href="https://napsterinblue.github.io/pages/about.html" title="About">About</a></li>
                    <li><a href="https://napsterinblue.github.io/archives.html" title="Archive">Archive</a></li>
                    <li><a href="https://napsterinblue.github.io/pages/resources.html" title="Resources">Resources</a></li>
                    <li><a href="https://napsterinblue.github.io/notes/" title="Notes">My Notes</a></li>

                </ul>
            </div>
        </div>
    </nav>


    
    <div class="container">
        <div class="row">
            <div class="col-sm-12">

                 


<article>
  <div class="technical_note">
  <header>
    <h1 class="technical_note_title">Word2Vec</h1>
    <div class="technical_note_date">
      <time datetime=" 2018-10-24T00:00:00Z "> 24 Oct 2018</time>
    </div>
  </header>
  <div class="content">
  

<p>As mentioned in the Word Embeddings notebook, there are many ways to train a Neural Network to produce a Word Embedding matrix for a given vocabulary.</p>

<p>One of the more popular implementations of this is <a href="https://www.tensorflow.org/tutorials/representation/word2vec">TensorFlow&rsquo;s Word2Vec</a>. This notebook should provide a high-level intuition of this training approach.</p>

<h2 id="fake-task">Fake Task</h2>

<p>The key takeaway for understanding how we fit an embedding layer is that we set our data up to solve an arbitrary problem when iterating over a corpus of text. For instance:</p>

<blockquote>
<p>Given a random context word, <code>c</code>, how likely is it that we&rsquo;ll see a target word, <code>t</code>, within <code>N</code> spaces from the context?</p>
</blockquote>

<p>Visually, this looks like the following</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">IPython.display</span> <span class="kn">import</span> <span class="n">Image</span>

<span class="n">Image</span><span class="p">(</span><span class="s1">&#39;images/word2vec_task.png&#39;</span><span class="p">)</span></code></pre></div>
<p><img src="word2vec_4_0.png" alt="png" /></p>

<p>Because this spits out to a softmax classifier, the outputs are calculated as</p>

<p>$Pr(t\vert c) = \frac{e^{\Theta^T e_c}}{\sum_j e^{\Theta^T_j e_c}}$</p>

<p>This means that we&rsquo;re learning both the Embedding Matrix, <code>E</code>, and some throw-away matrix, <code>Theta</code>, that we won&rsquo;t use after training.</p>

<p>However, this gets tricky when considering compute costs. Even in a 1000-word vocabulary (considered pretty small for text data), we&rsquo;re doing a <strong>ton</strong> of summing in these denominator terms, then again when calculating the cost function</p>

<p>$\mathcal{L}(\hat{y}, y) = - \sum y_i log(\hat{y_i})$</p>

<p>Not ideal. Instead we might try&hellip;</p>

<h3 id="negative-sampling">Negative Sampling</h3>

<p>Same idea. We want to build a fake task with the ultimate goal of extracting an Embedding Matrix, <code>E</code>.</p>

<p>Except this time, we&rsquo;re going to be more thoughtful about the way we sample our training data. Consider a sentence</p>

<pre><code>The quick brown fox jumped over the lazy dog
</code></pre>

<ul>
<li>We&rsquo;d start, as before, by picking a context word: <code>fox</code></li>
<li>Then we pick a random word within <code>N</code> spaces from it. We&rsquo;ll say <code>dog</code>.</li>

<li><p>Now (this is where it&rsquo;s different), we&rsquo;ll generate <code>k</code> random words from our vocabulary <em>that aren&rsquo;t within <code>N</code> of <code>fox</code></em></p>

<p>apple, door, nebraska, eerie, his</p></li>

<li><p>Finally we construct tuples of the form</p>

<p>(fox, dog, 1)
(fox, apple, 0)
(fox, door, 0)
(fox, nebraska, 0)
(fox, eerie, 0)
(fox, his, 0)</p></li>
</ul>

<p>to optimize the equation</p>

<p>$Pr(y=1 \vert c, t) = \sigma(\Theta^t e_c)$</p>

<p>Or &ldquo;given this context word and this taret word, how likely is it that they&rsquo;re within proximity of one another?&rdquo;</p>

<p>So instead of training on <code>10k</code> values in our softmax layer each step, we&rsquo;re only training <code>k+1</code> binary classification tasks at a time.</p>

</div>
  <aside>
      <div class="bug_reporting">
          <h4>Find an error or bug?</h4>
          <p>Everything on this site is available on GitHub. Head to <a href='https://github.com/napsterinblue/notes/issues/new'>and submit a suggested change</a>. You can also message me directly on <a href='https://twitter.com/napsterinblue'>Twitter</a>.</p>
      </div>
      </aside>

    </div>
</article>




            </div>

        </div>
    </div>

    

    <footer class="footer text-center">
        <div class="container">
            <span class="text-muted">This project contains 113 pages and is available on <a href="https://github.com/napsterinblue/notes">GitHub</a>. Copyright &copy; NapsterInBlue, <time datetime="2018">2018</time>.</span>
        </div>
    </footer>

    <script src="https://code.jquery.com/jquery-3.2.1.slim.min.js" integrity="sha384-KJ3o2DKtIkvYIK3UENzmM7KCkRr/rE9/Qpg6aAZGJwFDMVNA/GpGFF93hXpG5KkN"
        crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.12.3/umd/popper.min.js" integrity="sha384-vFJXuSJphROIrBnz7yo7oB41mKfc8JzQZiCq4NCceLEaO4IHwicKwpJf9c9IpFgh"
        crossorigin="anonymous"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0-beta.2/js/bootstrap.min.js" integrity="sha384-alpBpkh1PFOepccYVYDB4do5UnbKysX5WZXm3XxPqe5iKTfUKjNkCk9SaVuEZflJ"
        crossorigin="anonymous"></script>

</body>

</html>
