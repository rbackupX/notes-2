<!DOCTYPE html>
<html lang="en">

<head>

    
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-66582-32"></script>
    <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-66582-32');
    </script>

    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<meta property="og:title" content="Forward Propagation" />
<meta property="og:description" content="Forward propogation in a Neural Network is just an extrapolation of how we worked with Logistic Regression, where the caluculation chain just looked like
from IPython.display import ImageImage(&#39;images/logit.PNG&#39;) Our equation before,
$\hat{y} = w^{T} X &#43; b$
was much simpler in the sense that:
 X was an n x m vector (n features, m training examples) This was matrix-multiplied by w an n x 1 vector of weights (n because we want a weight per feature) Then we broadcast-added b Until we wound up with an m x 1 vector of predictions  A Different Curse of Dimensionality Now when we get into Neural Networks, with multiple-dimension matrix-multiplication to go from layer to layer, things can get pretty hairy." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://napsterinblue.github.io/notes/machine_learning/neural_nets/forward_prop/" />



<meta property="article:published_time" content="2018-08-10T00:00:00&#43;00:00"/>

<meta property="article:modified_time" content="2018-08-10T00:00:00&#43;00:00"/>











<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Forward Propagation"/>
<meta name="twitter:description" content="Forward propogation in a Neural Network is just an extrapolation of how we worked with Logistic Regression, where the caluculation chain just looked like
from IPython.display import ImageImage(&#39;images/logit.PNG&#39;) Our equation before,
$\hat{y} = w^{T} X &#43; b$
was much simpler in the sense that:
 X was an n x m vector (n features, m training examples) This was matrix-multiplied by w an n x 1 vector of weights (n because we want a weight per feature) Then we broadcast-added b Until we wound up with an m x 1 vector of predictions  A Different Curse of Dimensionality Now when we get into Neural Networks, with multiple-dimension matrix-multiplication to go from layer to layer, things can get pretty hairy."/>
<meta name="generator" content="Hugo 0.40.3" />

    
<script type="application/ld+json">
{
  "@context": "http://schema.org",
  "@type": "BlogPosting",
  "headline": "Forward Propagation",
  "url": "https://napsterinblue.github.io/notes/machine_learning/neural_nets/forward_prop/",
  "wordCount": "567",
  "datePublished": "2018-08-10T00:00:00&#43;00:00",
  "dateModified": "2018-08-10T00:00:00&#43;00:00",
  "author": {
    "@type": "Person",
    "name": ""
  }
}
</script> 

    <title>Forward Propagation</title>

    
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0-beta.2/css/bootstrap.min.css" integrity="sha384-PsH8R72JQ3SOdhVi3uxftmaW6Vc51MKb0q5P2rRUpPvrszuE4W1povHYgTpBfshb"
        crossorigin="anonymous">

    
    <link href="https://napsterinblue.github.io/notes/css/custom.css" rel="stylesheet">
    <link href="https://napsterinblue.github.io/notes/css/syntax.css" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css?family=Muli:400,500,700" rel="stylesheet">

    <link href="" rel="alternate" type="application/rss+xml" title="Data Science Notes" />

    <script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_CHTML">
</script>
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [['$','$'], ['\\(','\\)']],
    displayMath: [['$$','$$'], ['\[','\]']],
    processEscapes: true,
    processEnvironments: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre','code'],
    TeX: { equationNumbers: { autoNumber: "AMS" },
         extensions: ["AMSmath.js", "AMSsymbols.js"] }
  }
});
</script>

</head>

<body>

    <nav class="navbar navbar-expand-sm fixed-top">
        <div class="container-fluid">
            <a class="navbar-brand" href="https://napsterinblue.github.io">Movies, Metrics, Musings</a>
            <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarSupportedContent" aria-controls="navbarSupportedContent"
                aria-expanded="false" aria-label="Toggle navigation">
                <span class="navbar-toggler-icon"></span>
            </button>

            <div class="collapse navbar-collapse" id="navbarSupportedContent">
                <ul class="nav navbar-nav mr-auto"></ul>
                <ul class="nav navbar-nav">
                    <li><a href="https://napsterinblue.github.io/pages/about.html" title="About">About</a></li>
                    <li><a href="https://napsterinblue.github.io/archives.html" title="Archive">Archive</a></li>
                    <li><a href="https://napsterinblue.github.io/pages/resources.html" title="Resources">Resources</a></li>
                    <li><a href="https://napsterinblue.github.io/notes/" title="Notes">My Notes</a></li>

                </ul>
            </div>
        </div>
    </nav>


    
    <div class="container">
        <div class="row">
            <div class="col-sm-12">

                 


<article>
  <div class="technical_note">
  <header>
    <h1 class="technical_note_title">Forward Propagation</h1>
    <div class="technical_note_date">
      <time datetime=" 2018-08-10T00:00:00Z "> 10 Aug 2018</time>
    </div>
  </header>
  <div class="content">
  

<p>Forward propogation in a Neural Network is just an extrapolation of how we worked with <a href="https://napsterinblue.github.io/notes/#ml_regression">Logistic Regression</a>, where the caluculation chain just looked like</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">IPython.display</span> <span class="kn">import</span> <span class="n">Image</span></code></pre></div><div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="n">Image</span><span class="p">(</span><span class="s1">&#39;images/logit.PNG&#39;</span><span class="p">)</span></code></pre></div>
<p><img src="forward_prop_3_0.png" alt="png" /></p>

<p>Our equation before,</p>

<p>$\hat{y} = w^{T} X + b$</p>

<p>was much simpler in the sense that:</p>

<ul>
<li><code>X</code> was an <code>n x m</code> vector (<code>n</code> features, <code>m</code> training examples)</li>
<li>This was matrix-multiplied by <code>w</code> an <code>n x 1</code> vector of weights (<code>n</code> because we want a weight per feature)</li>
<li>Then we broadcast-added <code>b</code></li>
<li>Until we wound up with an <code>m x 1</code> vector of predictions</li>
</ul>

<h2 id="a-different-curse-of-dimensionality">A Different Curse of Dimensionality</h2>

<p>Now when we get into Neural Networks, with multiple-dimension matrix-multiplication to go from layer to layer, things can get pretty hairy.</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="n">Image</span><span class="p">(</span><span class="s1">&#39;images/dimensions.PNG&#39;</span><span class="p">)</span></code></pre></div>
<p><img src="forward_prop_8_0.png" alt="png" /></p>

<h3 id="terminology">Terminology</h3>

<ul>
<li>Our <strong>input layer</strong> <code>X</code> is still <code>n x m</code></li>
<li>Our <strong>output layer</strong> is still <code>m x 1</code>.</li>
<li><strong>Hidden/Activation layers</strong> are the nodes organized vertically that represent intermediate calculations.

<ul>
<li>The superscript represents which layer a node falls in</li>
<li>The subscript is which particular node you&rsquo;re referencing</li>
</ul></li>
<li>The <strong>weights matricies</strong> are the values that take you from one layer to the next via matrix multiplication.

<ul>
<li>*PAY CAREFUL ATTENTION TO THE FACT that <strong>W1 takes you from layer 1 to layer 2</strong>*</li>
</ul></li>
</ul>

<h3 id="keeping-the-dimensions-straight">Keeping the Dimensions Straight</h3>

<p>Always refer back to the fact that dot-producting two matricies along a central dimension cancels it out. For instance:</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="n">Image</span><span class="p">(</span><span class="s1">&#39;images/cancelling.png&#39;</span><span class="p">)</span></code></pre></div>
<p><img src="forward_prop_13_0.png" alt="png" /></p>

<p>Therefore, understanding which dimension your data should be in is an exercise in plugging all of the gaps to get you from <code>X</code> to <code>y</code></p>

<h4 id="w1">W1</h4>

<p>Getting to <code>a2</code> means following the equation</p>

<p>$a^{[2]} = W^{[1]}X$</p>

<p>As far as dimensions go, we&rsquo;re looking at</p>

<ul>
<li><code>X</code>: <code>n x m</code></li>
<li><code>a1</code>: <code>4 x m</code></li>
</ul>

<p>Subbing the <em>dimensions</em> in for the variables, we can start to fill in the gaps</p>

<p>$(4, m) = (?, ??) (n, m)$</p>

<p>because we know that we want <code>4</code> as the first value</p>

<p>$(4, m) = (4, ??) (n, m)$</p>

<p>we just need</p>

<p>$(4, m) = (4, n) (n, m)$</p>

<p>Thus</p>

<p>$dim_{W} = (4, n)$</p>

<h3 id="more-generally">More Generally</h3>

<p>If layer <code>j</code> is <code>m</code>-dimensional and layer <code>j+1</code> is <code>n</code>-dimensional</p>

<p>$W^{j} \quad \text{(which maps from j to j+1) has dimensionality} \quad (n \times m)$</p>

<h2 id="vectorizing-the-implementation">Vectorizing the Implementation</h2>

<p>The following image (grabbed from <em>Computing a Neural Network&rsquo;s Output</em> in Week 3) is as busy as it is informative.</p>

<p>It color-codes the same simple as above, highlighting the stacking approach to go from various vectors (e.g. <code>z[1]1, z[1]2, z[1]3, z[1]4</code>) to one large, unified matrix of values (<code>Z[1]</code>)</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="n">Image</span><span class="p">(</span><span class="s1">&#39;images/vectorizing.png&#39;</span><span class="p">)</span></code></pre></div>
<p><img src="forward_prop_27_0.png" alt="png" /></p>

<p>And so the process becomes 4 simple equations for one training example</p>

<p>$z^{[1]} = W^{[1]} x + b^{[1]}$</p>

<p>$a^{[1]} = sigmoid(z^{[1]})$</p>

<p>$z^{[2]} = W^{[2]} a^{[1]} + b^{[2]}$</p>

<p>$a^{[2]} = sigmoid(z^{[2]})$</p>

<p>In Python</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="n">z1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">W1</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span> <span class="o">+</span> <span class="n">b1</span>
<span class="n">a1</span> <span class="o">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">z1</span><span class="p">)</span>
<span class="n">z2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">W2</span><span class="p">,</span> <span class="n">a1</span><span class="p">)</span> <span class="o">+</span> <span class="n">b2</span>
<span class="n">a2</span> <span class="o">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">z2</span><span class="p">)</span></code></pre></div>
<p>If you want to extend to multiple training examples, you introduce a <code>(i)</code> notation, where</p>

<p>$a_{2}^{<a href="i">1</a>}$</p>

<p>refers to the <code>2nd</code> node activation, in the <code>1st</code> hidden layer, of the <code>ith</code> training example. And propogating for each prediction involves a big <code>for</code> loop</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">)):</span>
    <span class="n">z1</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">W1</span><span class="p">,</span> <span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="p">])</span> <span class="o">+</span> <span class="n">b1</span>
    <span class="n">a1</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">z1</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
    <span class="n">z2</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">W2</span><span class="p">,</span> <span class="n">a1</span><span class="p">[</span><span class="n">i</span><span class="p">])</span> <span class="o">+</span> <span class="n">b2</span>
    <span class="n">a2</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">z2</span><span class="p">[</span><span class="n">i</span><span class="p">])</span></code></pre></div>
<p>Or less-awfully, we can vectorize the whole thing</p>

<p>$Z^{[1]} = W^{[1]}X + b^{[1]}$</p>

<p>$A^{[1]} = sigmoid(Z^{[1]})$</p>

<p>$Z^{[2]} = W^{[2]}A^{1} + b^{[2]}$</p>

<p>$A^{[2]} = sigmoid(Z^{[2]})$</p>

<p>In Python</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="n">Z1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">W1</span><span class="p">,</span> <span class="n">X</span><span class="p">)</span> <span class="o">+</span> <span class="n">b1</span>
<span class="n">A1</span> <span class="o">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">Z1</span><span class="p">)</span>
<span class="n">Z2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">W2</span><span class="p">,</span> <span class="n">A1</span><span class="p">)</span> <span class="o">+</span> <span class="n">b2</span>
<span class="n">A2</span> <span class="o">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">Z2</span><span class="p">)</span></code></pre></div>
</div>
  <aside>
      <div class="bug_reporting">
          <h4>Find an error or bug?</h4>
          <p>Everything on this site is available on GitHub. Head to <a href='https://github.com/napsterinblue/notes/issues/new'>and submit a suggested change</a>. You can also message me directly on <a href='https://twitter.com/napsterinblue'>Twitter</a>.</p>
      </div>
      </aside>

    </div>
</article>




            </div>

        </div>
    </div>

    

    <footer class="footer text-center">
        <div class="container">
            <span class="text-muted">This project contains 113 pages and is available on <a href="https://github.com/napsterinblue/notes">GitHub</a>. Copyright &copy; NapsterInBlue, <time datetime="2018">2018</time>.</span>
        </div>
    </footer>

    <script src="https://code.jquery.com/jquery-3.2.1.slim.min.js" integrity="sha384-KJ3o2DKtIkvYIK3UENzmM7KCkRr/rE9/Qpg6aAZGJwFDMVNA/GpGFF93hXpG5KkN"
        crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.12.3/umd/popper.min.js" integrity="sha384-vFJXuSJphROIrBnz7yo7oB41mKfc8JzQZiCq4NCceLEaO4IHwicKwpJf9c9IpFgh"
        crossorigin="anonymous"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0-beta.2/js/bootstrap.min.js" integrity="sha384-alpBpkh1PFOepccYVYDB4do5UnbKysX5WZXm3XxPqe5iKTfUKjNkCk9SaVuEZflJ"
        crossorigin="anonymous"></script>

</body>

</html>
