<!DOCTYPE html>
<html lang="en">

<head>

    
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-66582-32"></script>
    <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-66582-32');
    </script>

    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<meta property="og:title" content="PCA: Principal Component Analysis" />
<meta property="og:description" content="Overview Principal Component Analysis is a technique used to extract one or more dimensions that capture as much of the variation of data as possible.
Intuition Following along with this YouTube video, say we have some data points that look like the following.
from IPython.display import Image Image(&#39;images/pca_yt_1.png&#39;) Most of the variation can be explained, not by x or y alone, but a combination of the two.
Image(&#39;images/pca_yt_2.png&#39;) Indeed, if you rotate the image relative to this axis, the data looks much more organized, with the most variance explained by our &ldquo;new x axis&rdquo;, and the second-most variance explained by our &ldquo;new y axis." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://napsterinblue.github.io/notes/stats/techniques/pca/" />



<meta property="article:published_time" content="2018-11-15T00:00:00&#43;00:00"/>

<meta property="article:modified_time" content="2018-11-15T00:00:00&#43;00:00"/>











<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="PCA: Principal Component Analysis"/>
<meta name="twitter:description" content="Overview Principal Component Analysis is a technique used to extract one or more dimensions that capture as much of the variation of data as possible.
Intuition Following along with this YouTube video, say we have some data points that look like the following.
from IPython.display import Image Image(&#39;images/pca_yt_1.png&#39;) Most of the variation can be explained, not by x or y alone, but a combination of the two.
Image(&#39;images/pca_yt_2.png&#39;) Indeed, if you rotate the image relative to this axis, the data looks much more organized, with the most variance explained by our &ldquo;new x axis&rdquo;, and the second-most variance explained by our &ldquo;new y axis."/>
<meta name="generator" content="Hugo 0.40.3" />

    
<script type="application/ld+json">
{
  "@context": "http://schema.org",
  "@type": "BlogPosting",
  "headline": "PCA: Principal Component Analysis",
  "url": "https://napsterinblue.github.io/notes/stats/techniques/pca/",
  "wordCount": "1271",
  "datePublished": "2018-11-15T00:00:00&#43;00:00",
  "dateModified": "2018-11-15T00:00:00&#43;00:00",
  "author": {
    "@type": "Person",
    "name": ""
  }
}
</script> 

    <title>PCA: Principal Component Analysis</title>

    
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0-beta.2/css/bootstrap.min.css" integrity="sha384-PsH8R72JQ3SOdhVi3uxftmaW6Vc51MKb0q5P2rRUpPvrszuE4W1povHYgTpBfshb"
        crossorigin="anonymous">

    
    <link href="https://napsterinblue.github.io/notes/css/custom.css" rel="stylesheet">
    <link href="https://napsterinblue.github.io/notes/css/syntax.css" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css?family=Muli:400,500,700" rel="stylesheet">

    <link href="" rel="alternate" type="application/rss+xml" title="Data Science Notes" />

    <script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_CHTML">
</script>
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [['$','$'], ['\\(','\\)']],
    displayMath: [['$$','$$'], ['\[','\]']],
    processEscapes: true,
    processEnvironments: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre','code'],
    TeX: { equationNumbers: { autoNumber: "AMS" },
         extensions: ["AMSmath.js", "AMSsymbols.js"] }
  }
});
</script>

</head>

<body>

    <nav class="navbar navbar-expand-sm fixed-top">
        <div class="container-fluid">
            <a class="navbar-brand" href="https://napsterinblue.github.io">Movies, Metrics, Musings</a>
            <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarSupportedContent" aria-controls="navbarSupportedContent"
                aria-expanded="false" aria-label="Toggle navigation">
                <span class="navbar-toggler-icon"></span>
            </button>

            <div class="collapse navbar-collapse" id="navbarSupportedContent">
                <ul class="nav navbar-nav mr-auto"></ul>
                <ul class="nav navbar-nav">
                    <li><a href="https://napsterinblue.github.io/pages/about.html" title="About">About</a></li>
                    <li><a href="https://napsterinblue.github.io/archives.html" title="Archive">Archive</a></li>
                    <li><a href="https://napsterinblue.github.io/pages/resources.html" title="Resources">Resources</a></li>
                    <li><a href="https://napsterinblue.github.io/notes/" title="Notes">My Notes</a></li>

                </ul>
            </div>
        </div>
    </nav>


    
    <div class="container">
        <div class="row">
            <div class="col-sm-12">

                 


<article>
  <div class="technical_note">
  <header>
    <h1 class="technical_note_title">PCA: Principal Component Analysis</h1>
    <div class="technical_note_date">
      <time datetime=" 2018-11-15T00:00:00Z "> 15 Nov 2018</time>
    </div>
  </header>
  <div class="content">
  

<h2 id="overview">Overview</h2>

<p><strong>Principal Component Analysis</strong> is a technique used to extract one or more dimensions that capture as much of the variation of data as possible.</p>

<h3 id="intuition">Intuition</h3>

<p>Following along with <a href="https://www.youtube.com/watch?v=_UVHneBUBW0">this YouTube video</a>, say we have some data points that look like the following.</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">IPython.display</span> <span class="kn">import</span> <span class="n">Image</span>

<span class="n">Image</span><span class="p">(</span><span class="s1">&#39;images/pca_yt_1.png&#39;</span><span class="p">)</span></code></pre></div>
<p><img src="pca_5_0.png" alt="png" /></p>

<p>Most of the variation can be explained, not by <code>x</code> or <code>y</code> alone, but a combination of the two.</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="n">Image</span><span class="p">(</span><span class="s1">&#39;images/pca_yt_2.png&#39;</span><span class="p">)</span></code></pre></div>
<p><img src="pca_7_0.png" alt="png" /></p>

<p>Indeed, if you rotate the image relative to this axis, the data looks much more organized, with the most variance explained by our &ldquo;new <code>x</code> axis&rdquo;, and the second-most variance explained by our &ldquo;new <code>y</code> axis.&rdquo;</p>

<p>These new axes are our <strong>Principal Components</strong>.</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="n">Image</span><span class="p">(</span><span class="s1">&#39;images/pca_yt_3.png&#39;</span><span class="p">)</span></code></pre></div>
<p><img src="pca_9_0.png" alt="png" /></p>

<p>If you consider that this data actually represents individual Cells represented by a number of Genes, you can map each Gene&rsquo;s value to how much it influenced a Principal Component. High values get plotted further right. Low values get plotted further left.</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="n">Image</span><span class="p">(</span><span class="s1">&#39;images/pca_yt_4.png&#39;</span><span class="p">)</span></code></pre></div>
<p><img src="pca_11_0.png" alt="png" /></p>

<p>So if we sequenced then reduced three different cells, and Cell 1 had similar gene values to Cell 1, we might be able to represent it across two Principal Components like so.</p>

<p>Note, that <em>the first Principal Component captures the most amount of variance within the data.</em> Followed by the second, then third, etc.</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="n">Image</span><span class="p">(</span><span class="s1">&#39;images/pca_yt_5.png&#39;</span><span class="p">)</span></code></pre></div>
<p><img src="pca_13_0.png" alt="png" /></p>

<h3 id="analysis-and-pca">Analysis and PCA</h3>

<p>For analytical purposes, PCA affords a few interesting benefits.</p>

<p>First, like above, similar Cells wind up clustering close to one another.</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="n">Image</span><span class="p">(</span><span class="s1">&#39;images/pca_yt_6.png&#39;</span><span class="p">)</span></code></pre></div>
<p><img src="pca_15_0.png" alt="png" /></p>

<p>Second, if we wanted to determine what makes &ldquo;Neural Cells&rdquo; fundamentally different from &ldquo;Blood Cells&rdquo; we could take a look at the &ldquo;Influence on PC2&rdquo; variable to identify the Genes that drive these cells to be clustered in different areas of the reduced space.</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="n">Image</span><span class="p">(</span><span class="s1">&#39;images/pca_yt_7.png&#39;</span><span class="p">)</span></code></pre></div>
<p><img src="pca_17_0.png" alt="png" /></p>

<p>Third, we can plot explained variance as a function of the number of Principal Components. This helps us identify when to stop reducing the dimensionality of our data.</p>

<h3 id="mechanics-with-fake-data">Mechanics with Fake Data</h3>

<p>So what does that mean in practice?</p>

<p>Following along with <a href="https://github.com/joelgrus/data-science-from-scratch/blob/871bb335ede8b1a104317fdf74e7a5cd485c184c/code/working_with_data.py#L215-L315">the dataset from Chapter 10</a> of Joel Grus&rsquo; <em>Data Science from Scratch</em>, we&rsquo;ve got a dataset that looks like the following.</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="o">%</span><span class="n">pylab</span> <span class="n">inline</span>

<span class="kn">import</span> <span class="nn">pickle</span>

<span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s1">&#39;data/pca_grus.pkl&#39;</span><span class="p">,</span> <span class="s1">&#39;rb&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">pickle</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">f</span><span class="p">)</span></code></pre></div>
<pre><code>Populating the interactive namespace from numpy and matplotlib
</code></pre>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">()</span>

<span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">])</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">([</span><span class="o">-</span><span class="mi">20</span><span class="p">,</span> <span class="mi">50</span><span class="p">])</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">([</span><span class="o">-</span><span class="mi">40</span><span class="p">,</span> <span class="mi">10</span><span class="p">]);</span></code></pre></div>
<p><img src="pca_23_0.png" alt="png" /></p>

<p>Again, most of the variation doesn&rsquo;t seem to correspond with either the <code>x</code> or <code>y</code> axis alone.</p>

<h3 id="normalization">Normalization</h3>

<p>Normalize your data such that each attribute has mean zero. Joel doesn&rsquo;t normalize using standard deviation. Though I&rsquo;ve seen others use it. Setting to <code>False</code> for consistency&rsquo;s sake.</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">StandardScaler</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">StandardScaler</span><span class="p">(</span><span class="n">with_std</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span></code></pre></div><div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]);</span></code></pre></div>
<p><img src="pca_28_0.png" alt="png" /></p>

<h3 id="finding-direction-that-maximizes-variance">Finding direction that maximizes variance</h3>

<p>As above, we want to find some straight line to draw through our data in a path that explains the most variance.</p>

<p>In essence, that means generating a ton of hypothetical lines from the origin, and picking the one that explains the most variance over our data.</p>

<p>So first, a helper function that we can use to rescale any vector <code>w</code> to have unit length.</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="k">def</span> <span class="nf">direction</span><span class="p">(</span><span class="n">w</span><span class="p">):</span>
    <span class="s2">&#34;&#34;&#34;Given a vector w, rescale it to have unit length.
</span><span class="s2">    The result will be an n-dimensional vector from the origin.&#34;&#34;&#34;</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">w</span><span class="p">)</span></code></pre></div>
<p>Then, we&rsquo;ll develop measures for explained variance (what we&rsquo;re optimizing over)</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="k">def</span> <span class="nf">directional_variance_i</span><span class="p">(</span><span class="n">x_i</span><span class="p">,</span> <span class="n">w</span><span class="p">):</span>
    <span class="s2">&#34;&#34;&#34;The dot product of a vector and a direction, squared tells
</span><span class="s2">    us how much variance a hypothetical axis gives us&#34;&#34;&#34;</span>
    <span class="n">dir_w</span> <span class="o">=</span> <span class="n">direction</span><span class="p">(</span><span class="n">w</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">x_i</span><span class="p">,</span> <span class="n">w</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span>

<span class="k">def</span> <span class="nf">directional_variance</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">w</span><span class="p">):</span>
    <span class="s2">&#34;&#34;&#34;Same as above, but the sum across all records in the data&#34;&#34;&#34;</span>
    <span class="k">return</span> <span class="nb">sum</span><span class="p">(</span><span class="n">directional_variance_i</span><span class="p">(</span><span class="n">x_i</span><span class="p">,</span> <span class="n">w</span><span class="p">)</span>
               <span class="k">for</span> <span class="n">x_i</span> <span class="ow">in</span> <span class="n">X</span><span class="p">)</span> </code></pre></div>
<p>Because this is an optimization, we&rsquo;ll also need a gradient function</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="k">def</span> <span class="nf">directional_variance_gradient</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">w</span><span class="p">):</span>
    <span class="s2">&#34;&#34;&#34;Implementation not important. Just calculus.&#34;&#34;&#34;</span>
    <span class="k">return</span> <span class="n">deriv_of_variance_fn</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">w</span><span class="p">)</span></code></pre></div>
<p>Finally, Joel pipes these into a gradient descent function</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="k">def</span> <span class="nf">first_principal_component</span><span class="p">(</span><span class="n">X</span><span class="p">):</span>
    <span class="n">guess</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="n">X</span><span class="p">[</span><span class="mi">0</span><span class="p">]]</span>                             <span class="c1"># init to 1</span>
    <span class="n">unscaled_maximizer</span> <span class="o">=</span> <span class="n">maximize_batch</span><span class="p">(</span>                  <span class="c1"># custom max fn he cooked up</span>
        <span class="n">cost_fn</span> <span class="o">=</span> <span class="n">directional_variance</span><span class="p">(</span><span class="n">X</span><span class="p">),</span>                <span class="c1"># see above</span>
        <span class="n">cost_gradient</span> <span class="o">=</span> <span class="n">directional_variance_gradient</span><span class="p">(</span><span class="n">X</span><span class="p">),</span> <span class="c1"># see above</span>
        <span class="n">guess</span><span class="p">)</span>                                            <span class="c1"># updated on each step of maximize_batch</span>
    <span class="k">return</span> <span class="n">direction</span><span class="p">(</span><span class="n">unscaled_maximizer</span><span class="p">)</span>                  <span class="c1"># normalized to unit length. Just a direction</span></code></pre></div>
<p>His result (we&rsquo;ll trust him) returns <code>[0.924, 0.383]</code>, which looks about right.</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="n">vec_x</span> <span class="o">=</span> <span class="mf">0.924</span>
<span class="n">vec_y</span> <span class="o">=</span> <span class="mf">0.383</span>

<span class="c1"># scaling the `dx` and `dy` args so the line shows up better</span>
<span class="n">scaling</span> <span class="o">=</span> <span class="mi">10</span>

<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]);</span>
<span class="n">plt</span><span class="o">.</span><span class="n">arrow</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
          <span class="n">dx</span><span class="o">=</span><span class="n">vec_x</span><span class="o">*</span><span class="n">scaling</span><span class="p">,</span>
          <span class="n">dy</span><span class="o">=</span><span class="n">vec_y</span><span class="o">*</span><span class="n">scaling</span><span class="p">,</span>
          <span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">)</span></code></pre></div>
<pre><code>&lt;matplotlib.patches.FancyArrow at 0x11ac9d30&gt;
</code></pre>

<p><img src="pca_38_1.png" alt="png" /></p>

<h3 id="project">Project</h3>

<p>Now we can project our data down onto the first Principal Component using a dot product.</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span></code></pre></div><div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="k">def</span> <span class="nf">project_one_vector</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="n">w</span><span class="p">):</span>
    <span class="n">w</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">w</span><span class="p">)</span>
    <span class="n">projection_length</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="n">w</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">projection_length</span> <span class="o">*</span> <span class="n">w</span></code></pre></div><div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="k">def</span> <span class="nf">project_matrix</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">w</span><span class="p">):</span>
    <span class="n">proj</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">project_one_vector</span><span class="p">(</span><span class="n">x_i</span><span class="p">,</span> <span class="n">w</span><span class="p">)</span>
                     <span class="k">for</span> <span class="n">x_i</span> <span class="ow">in</span> <span class="n">X</span><span class="p">])</span>
    <span class="k">return</span> <span class="n">proj</span></code></pre></div>
<p>Of course, projecting from 2D to 1D just means a straight line, lol</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="n">w</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.924</span><span class="p">,</span> <span class="mf">0.383</span><span class="p">]</span>
<span class="n">proj_X</span> <span class="o">=</span> <span class="n">project_matrix</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">w</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">proj_X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">proj_X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]);</span></code></pre></div>
<p><img src="pca_45_0.png" alt="png" /></p>

<h3 id="more-principal-components">More Principal Components</h3>

<p>On the flip side, if we had a higher-order dataset and wanted to find more Principal Components, we would <em>subtract</em> the projection from each vector, and re-run the same search for a principal component.</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="k">def</span> <span class="nf">remove_projection_one_vector</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="n">w</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">v</span> <span class="o">-</span> <span class="n">project_one_vector</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="n">w</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">remove_projection_matrix</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">w</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">remove_projection_one_vector</span><span class="p">(</span><span class="n">x_i</span><span class="p">,</span> <span class="n">w</span><span class="p">)</span>
                     <span class="k">for</span> <span class="n">x_i</span> <span class="ow">in</span> <span class="n">X</span><span class="p">])</span></code></pre></div>
<p>Here&rsquo;s the same data, transformed to remove our first component.</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="n">x_sans_proj</span> <span class="o">=</span> <span class="n">remove_projection_matrix</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">w</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x_sans_proj</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">x_sans_proj</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">(</span><span class="o">-</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">);</span></code></pre></div>
<p><img src="pca_50_0.png" alt="png" /></p>

<p>Re-running <code>first_principal_component</code> on this data would yield another Principal Component. At scale, we can iteratively gather and more components with the following:</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="k">def</span> <span class="nf">PCA</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">num_components</span><span class="p">):</span>
    <span class="n">components</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_components</span><span class="p">):</span>
        <span class="n">component</span> <span class="o">=</span> <span class="n">first_principal_component</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
        <span class="n">components</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">component</span><span class="p">)</span>
        <span class="n">X</span> <span class="o">=</span> <span class="n">remove_projection_matrix</span><span class="p">(</span><span class="n">component</span><span class="p">)</span>
        
    <span class="k">return</span> <span class="n">components</span></code></pre></div>
<p>Then after running this, we can transform our original dataset to it&rsquo;s lower-dimension representation with</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="k">def</span> <span class="nf">transform_vector</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="n">components</span><span class="p">):</span>
    <span class="k">return</span> <span class="p">[</span><span class="n">dot</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="n">w</span><span class="p">)</span> <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">components</span><span class="p">]</span>

<span class="k">def</span> <span class="nf">transform_matrix</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">components</span><span class="p">):</span>
    <span class="k">return</span> <span class="p">[</span><span class="n">transform_vector</span><span class="p">(</span><span class="n">x_i</span><span class="p">,</span> <span class="n">components</span><span class="p">)</span> <span class="k">for</span> <span class="n">x_i</span> <span class="ow">in</span> <span class="n">X</span><span class="p">]</span></code></pre></div>
<h2 id="on-a-real-dataset-w-sklearn">On a Real Dataset w sklearn</h2>

<p>Of course, after you&rsquo;ve got the concepts down, reaching for a neater implementation is usually a good idea.</p>

<p>We&rsquo;ll be using Paulo Corez&rsquo;s <a href="http://archive.ics.uci.edu/ml/machine-learning-databases/forest-fires/forestfires.names">Forest Fires dataset</a> to examine this concept.</p>

<p>This dataset is:</p>

<ul>
<li>Some X/Y coordinates representing location of the fire in the park</li>
<li>Month and Day columns of the fire</li>
<li>A bunch of numeric columns representing various metrics relevant to this study</li>
<li>A final <code>area</code> column outlining how much area the fire wound up spreading to.</li>
</ul>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">requests</span>
<span class="kn">from</span> <span class="nn">io</span> <span class="kn">import</span> <span class="n">StringIO</span>

<span class="kn">import</span> <span class="nn">pandas</span> <span class="kn">as</span> <span class="nn">pd</span></code></pre></div><div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="n">conn</span> <span class="o">=</span> <span class="n">requests</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;http://archive.ics.uci.edu/ml/machine-learning-databases/forest-fires/forestfires.csv&#39;</span><span class="p">)</span>

<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="n">StringIO</span><span class="p">(</span><span class="n">conn</span><span class="o">.</span><span class="n">text</span><span class="p">))</span></code></pre></div>
<p>For purposes of demonstration, the specifics of the variables is of little importance to us, as we&rsquo;ll be reducing them anyhow. Of course in practice, the resulting components should be human-interpretable.</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="n">df</span><span class="o">.</span><span class="n">head</span><span class="p">()</span></code></pre></div>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>X</th>
      <th>Y</th>
      <th>month</th>
      <th>day</th>
      <th>FFMC</th>
      <th>DMC</th>
      <th>DC</th>
      <th>ISI</th>
      <th>temp</th>
      <th>RH</th>
      <th>wind</th>
      <th>rain</th>
      <th>area</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>7</td>
      <td>5</td>
      <td>mar</td>
      <td>fri</td>
      <td>86.2</td>
      <td>26.2</td>
      <td>94.3</td>
      <td>5.1</td>
      <td>8.2</td>
      <td>51</td>
      <td>6.7</td>
      <td>0.0</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>7</td>
      <td>4</td>
      <td>oct</td>
      <td>tue</td>
      <td>90.6</td>
      <td>35.4</td>
      <td>669.1</td>
      <td>6.7</td>
      <td>18.0</td>
      <td>33</td>
      <td>0.9</td>
      <td>0.0</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>7</td>
      <td>4</td>
      <td>oct</td>
      <td>sat</td>
      <td>90.6</td>
      <td>43.7</td>
      <td>686.9</td>
      <td>6.7</td>
      <td>14.6</td>
      <td>33</td>
      <td>1.3</td>
      <td>0.0</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>8</td>
      <td>6</td>
      <td>mar</td>
      <td>fri</td>
      <td>91.7</td>
      <td>33.3</td>
      <td>77.5</td>
      <td>9.0</td>
      <td>8.3</td>
      <td>97</td>
      <td>4.0</td>
      <td>0.2</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>8</td>
      <td>6</td>
      <td>mar</td>
      <td>sun</td>
      <td>89.3</td>
      <td>51.3</td>
      <td>102.2</td>
      <td>9.6</td>
      <td>11.4</td>
      <td>99</td>
      <td>1.8</td>
      <td>0.0</td>
      <td>0.0</td>
    </tr>
  </tbody>
</table>
</div>

<p>We&rsquo;ll omit the categorical variables.</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="n">df_num</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">select_dtypes</span><span class="p">([</span><span class="s1">&#39;int64&#39;</span><span class="p">,</span> <span class="s1">&#39;float64&#39;</span><span class="p">])</span></code></pre></div>
<p>Scaling our data as before, we can see that we&rsquo;ve got 0 means.</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="n">df_num</span> <span class="o">=</span> <span class="n">StandardScaler</span><span class="p">()</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">df_num</span><span class="o">.</span><span class="n">values</span><span class="p">)</span>
<span class="n">df_num</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span></code></pre></div>
<pre><code>array([ 2.11307438e-16,  2.61127891e-16, -1.75230559e-15, -2.74871465e-17,
        6.87178661e-17,  1.03076799e-17,  2.54256105e-16,  2.19897172e-16,
       -4.19178983e-16, -6.87178661e-18,  4.12307197e-17])
</code></pre>

<p>This time, we&rsquo;ll import a simple <code>PCA</code> object from <code>sklearn.decomposition</code></p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">sklearn.decomposition</span> <span class="kn">import</span> <span class="n">PCA</span>
<span class="n">pca</span> <span class="o">=</span> <span class="n">PCA</span><span class="p">()</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">df_num</span><span class="p">)</span></code></pre></div>
<p>Investigating the <code>explained_variance_ratio_</code> property, we see that by about the 6th Principal Component, we can explain 90% of the variance.</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">cumsum</span><span class="p">(</span><span class="n">pca</span><span class="o">.</span><span class="n">explained_variance_ratio_</span><span class="p">));</span></code></pre></div>
<p><img src="pca_66_0.png" alt="png" /></p>

<p>And so we&rsquo;ll re-fit accordingly.</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="n">ideal_pca</span> <span class="o">=</span> <span class="n">PCA</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">6</span><span class="p">)</span>
<span class="n">df_num_6pcs</span> <span class="o">=</span> <span class="n">ideal_pca</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">df_num</span><span class="p">)</span></code></pre></div><div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="n">df_num</span><span class="o">.</span><span class="n">shape</span></code></pre></div>
<pre><code>(517, 11)
</code></pre>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="n">ideal_pca</span><span class="o">.</span><span class="n">components_</span><span class="o">.</span><span class="n">shape</span></code></pre></div>
<pre><code>(6, 11)
</code></pre>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="n">df_num_6pcs</span><span class="o">.</span><span class="n">shape</span></code></pre></div>
<pre><code>(517, 6)
</code></pre>

</div>
  <aside>
      <div class="bug_reporting">
          <h4>Find an error or bug?</h4>
          <p>Everything on this site is available on GitHub. Head to <a href='https://github.com/napsterinblue/notes/issues/new'>and submit a suggested change</a>. You can also message me directly on <a href='https://twitter.com/napsterinblue'>Twitter</a>.</p>
      </div>
      </aside>

    </div>
</article>




            </div>

        </div>
    </div>

    

    <footer class="footer text-center">
        <div class="container">
            <span class="text-muted">This project contains 113 pages and is available on <a href="https://github.com/napsterinblue/notes">GitHub</a>. Copyright &copy; NapsterInBlue, <time datetime="2018">2018</time>.</span>
        </div>
    </footer>

    <script src="https://code.jquery.com/jquery-3.2.1.slim.min.js" integrity="sha384-KJ3o2DKtIkvYIK3UENzmM7KCkRr/rE9/Qpg6aAZGJwFDMVNA/GpGFF93hXpG5KkN"
        crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.12.3/umd/popper.min.js" integrity="sha384-vFJXuSJphROIrBnz7yo7oB41mKfc8JzQZiCq4NCceLEaO4IHwicKwpJf9c9IpFgh"
        crossorigin="anonymous"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0-beta.2/js/bootstrap.min.js" integrity="sha384-alpBpkh1PFOepccYVYDB4do5UnbKysX5WZXm3XxPqe5iKTfUKjNkCk9SaVuEZflJ"
        crossorigin="anonymous"></script>

</body>

</html>
