{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Simple Optimization in TensorFlow\"\n",
    "date: 2018-09-13\n",
    "type: technical_note\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actually using TensorFlow to optimize/fit a model is similar to the workflow we outlined in the Basics section, but with a few crucial additions:\n",
    "\n",
    "- Placeholder variables for `X` and `y`\n",
    "- Defining a `loss` function\n",
    "- Select an `Optimizer` object you want to use\n",
    "- Make a `train` node that uses the `Optimizer` to minimize the `loss`\n",
    "- Run your `Session()` to fetch the `train` node, passing your placeholders `X` and `y` with `feed_dict`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Another Iris Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assuming comfort with [the general intuition of Logistic Regression](https://napsterinblue.github.io/notes/machine_learning/regression/logistic_regression_basics/), we'll spin up a trivial example to demonstrate setting up the probem in TensorFlow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 5.1,  3.5,  1.4,  0.2],\n",
       "       [ 4.9,  3. ,  1.4,  0.2],\n",
       "       [ 4.7,  3.2,  1.3,  0.2],\n",
       "       [ 4.6,  3.1,  1.5,  0.2],\n",
       "       [ 5. ,  3.6,  1.4,  0.2]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = load_iris()\n",
    "\n",
    "X = data.data\n",
    "X[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.,  1.,  0.,  0.,  0.,  0.])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = data.target\n",
    "y = (y == 0).astype(float)\n",
    "y[48:54]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 4) (150,)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use `tf.placeholder()` to slot out nodes we'll use to pass in observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, shape=[None, 4])\n",
    "y_true = tf.placeholder(tf.float32, shape=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `w`eights and `b`ias terms will update in each iteration. We'll initialize them to zeros and let TensorFlow do the rest.\n",
    "\n",
    "`y_pred` leverages `w` and `b` at each step, applying the sigmoid function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w = tf.Variable([[0, 0, 0, 0]], dtype=tf.float32, name='weights')\n",
    "b = tf.Variable(0, dtype=tf.float32, name='bias')\n",
    "\n",
    "y_pred = tf.sigmoid(tf.matmul(w, tf.transpose(x)) + b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to define a `loss` function for TensorFlow to evaluate against.\n",
    "\n",
    "The most popular cost function for classification is `tf.nn.sigmoid_cross_entropy_with_logits`, with `labels` set to your targets and `logits` the node/placeholder in your execution graph.\n",
    "\n",
    "The `reduce_mean()` gives us the \"one over `m`, times the sum\" leading value in our cost function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss = tf.nn.sigmoid_cross_entropy_with_logits(labels=y_true, logits=y_pred)\n",
    "loss = tf.reduce_mean(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finaly, we define an optimization strategy and use that to build a `train` node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.5\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=0.5)\n",
    "train = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execute the graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All told, actually running this model requires initializing the global variables and a call to `tf.Session().run()` to fetch the `train` node, **passing in our training observations**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [array([[-0.49999967, -0.49999914, -0.49999964, -0.49999908]], dtype=float32), -0.49999782]\n",
      "5 [array([[-1.63306212, -1.6268971 , -1.63869369, -1.6400671 ]], dtype=float32), -1.6299324]\n",
      "10 [array([[-2.16804147, -2.15893531, -2.17636108, -2.1783905 ]], dtype=float32), -2.1634178]\n",
      "15 [array([[-2.47912383, -2.4683075 , -2.48900652, -2.49141765]], dtype=float32), -2.4736314]\n",
      "20 [array([[-2.66975784, -2.65789342, -2.68059874, -2.68324351]], dtype=float32), -2.6637332]\n",
      "10 [array([[-2.76915216, -2.75674129, -2.78049254, -2.78325939]], dtype=float32), -2.7628503]\n"
     ]
    }
   ],
   "source": [
    "NUM_STEPS = 25\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for step in range(NUM_STEPS):\n",
    "        sess.run(train, feed_dict={x: X, y_true: y})\n",
    "        \n",
    "        if step % 5 == 0:\n",
    "            print(step, sess.run([w, b]))\n",
    "            \n",
    "    print(10, sess.run([w, b]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Printing the weights we can see that within a few quick iterations the model is already learning better values for our features to minimize loss."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
